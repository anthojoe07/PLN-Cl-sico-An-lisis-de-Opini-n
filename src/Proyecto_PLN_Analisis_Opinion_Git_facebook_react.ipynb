{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de Opinión en Repositorios de Software (GitHub)\n",
        "**Proyecto:** Procesamiento de Lenguaje Natural (PLN)\n",
        "**Repositorio Analizado:** `facebook/react`\n",
        "\n",
        "\n",
        "**Autores:**\n",
        "\n",
        "\n",
        " * Karla Patricia Solorzano Parra\n",
        " * Anthony Joel Toalombo Aisabucha\n",
        " * Mayerly Kristel Velos Alburquerque\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Objetivos del Proyecto\n",
        "El objetivo es construir un observatorio de salud de proyectos Open Source analizando los comentarios, issues y pull requests. Se busca:\n",
        "1.  **Recolectar datos reales** usando la API de GitHub.\n",
        "2.  **Detectar sentimientos** (Positivo, Negativo, Neutral) para medir la \"temperatura\" de la comunidad.\n",
        "3.  **Identificar temas ocultos** (Clustering) para saber de qué se habla más (Bugs, Documentación, Features)."
      ],
      "metadata": {
        "id": "yC8I2f5eKbIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Librerías de NLP (NLTK)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Librerías de Machine Learning (Scikit-Learn)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Configuración\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Descargar diccionarios\n",
        "print(\"Cargando herramientas de lenguaje...\")\n",
        "for dep in ['stopwords', 'wordnet', 'omw-1.4', 'vader_lexicon']:\n",
        "    try: nltk.data.find(f'corpora/{dep}')\n",
        "    except: nltk.download(dep, quiet=True)\n",
        "print(\" Todo listo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqdikeKzD72k",
        "outputId": "9da18e5f-0243-4a9c-a24a-6459e39549e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando herramientas de lenguaje...\n",
            " Todo listo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2. Ingesta de Datos (Data Ingestion)\n",
        "Conexión segura a la API de GitHub para descargar issues y comentarios.\n",
        "* **Seguridad:** Se utiliza `google.colab.userdata` para proteger el Token de acceso.\n",
        "* **Volumen:** Se descargarán **30 páginas** (aprox. 900 registros) para garantizar representatividad estadística."
      ],
      "metadata": {
        "id": "fu7wDhfvKj_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Token seguro desde Secrets ---\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    print(\"Token cargado correctamente desde Secrets.\")\n",
        "except:\n",
        "    # Si falla, no rompemos el código, solo avisamos\n",
        "    GITHUB_TOKEN = None\n",
        "    print(\" No se encontró 'GITHUB_TOKEN'. Se usará modo limitado.\")\n",
        "\n",
        "REPO_OWNER = \"facebook\"\n",
        "REPO_NAME = \"react\"\n",
        "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"} if GITHUB_TOKEN else {}\n",
        "\n",
        "PAGES_TO_FETCH = 30\n",
        "\n",
        "def get_comments(comments_url):\n",
        "    if not comments_url: return \"\"\n",
        "    try:\n",
        "        response = requests.get(comments_url, headers=HEADERS)\n",
        "        if response.status_code == 200:\n",
        "            return \" \".join([c['body'] for c in response.json() if c.get('body')])\n",
        "    except: pass\n",
        "    return \"\"\n",
        "\n",
        "print(f\"--- Conectando a GitHub para descargar datos de {REPO_NAME} ---\")\n",
        "\n",
        "all_data = []\n",
        "for page in range(1, PAGES_TO_FETCH + 1):\n",
        "    print(f\"Descargando página {page}...\", end=\"\\r\")\n",
        "    url = f\"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues?state=all&per_page=30&page={page}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        for item in response.json():\n",
        "            comentarios = \"\"\n",
        "            if item['comments'] > 0:\n",
        "                comentarios = get_comments(item['comments_url'])\n",
        "                time.sleep(0.1) # Pausa pequeña\n",
        "\n",
        "            all_data.append({\n",
        "                \"id\": item['number'],\n",
        "                \"type\": \"pull_request\" if \"pull_request\" in item else \"issue\",\n",
        "                \"title\": item['title'],\n",
        "                \"body\": item.get('body', \"\") or \"\",\n",
        "                \"comments\": comentarios,\n",
        "                \"state\": item['state']\n",
        "            })\n",
        "        time.sleep(0.5)\n",
        "    except Exception as e:\n",
        "        print(f\"Error en página {page}: {e}\")\n",
        "\n",
        "# Guardar CSV\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Creamos la columna 'full_text' que usaremos abajo\n",
        "df['full_text'] = df['title'].fillna('') + \" \" + df['body'].fillna('') + \" \" + df['comments'].fillna('')\n",
        "\n",
        "df.to_csv(\"dataset_react.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"\\n¡ÉXITO! Se creó el archivo 'dataset_react.csv' con {len(df)} registros.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-Jyxy2lD619",
        "outputId": "9739ecea-4044-4dd0-c7c0-0ad0c4b7795c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token cargado correctamente desde Secrets.\n",
            "--- Conectando a GitHub para descargar datos de react ---\n",
            "\n",
            "¡ÉXITO! Se creó el archivo 'dataset_react.csv' con 900 registros.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocesamiento y Normalización (NLP Pipeline)\n",
        "Los datos textuales crudos contienen ruido que afecta el desempeño de los modelos. Se aplica un pipeline de limpieza estructurado:\n",
        "1.  **Normalización:** Estandarización a minúsculas (`lowercase`).\n",
        "2.  **Filtrado de Ruido:** Eliminación de artefactos web (URLs, menciones, etiquetas HTML).\n",
        "3.  **Stopwords Removal:** Descarte de términos funcionales sin carga semántica (artículos, preposiciones).\n",
        "4.  **Lematización:** Reducción morfológica de las palabras a su raíz (lema) utilizando el corpus de **WordNet**."
      ],
      "metadata": {
        "id": "AZMa0NO7Koc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPGu6xuO4RUI",
        "outputId": "c9d99732-cd7b-4054-caae-0eb38565cea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando dataset...\n",
            "Limpiando textos...\n",
            "\n",
            "--- RESULTADO ---\n",
            "Original: Bump webpack from 5.82.1 to 5.104.1 Bumps [webpack](https://github.com/webpack/w\n",
            "Limpio:   bump webpack bump webpack detail summaryrelease notessummary pemsourced href rel\n",
            "\n",
            "¡Archivo 'dataset_react_procesado.csv' guardado correctamente!\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['would', 'could', 'issue', 'pull', 'request', 'react', 'code'])\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    if not isinstance(texto, str):\n",
        "        return \"\"\n",
        "\n",
        "    # a. Todo a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # b. Eliminar URLs y menciones\n",
        "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
        "    texto = re.sub(r'@\\w+', '', texto)\n",
        "\n",
        "    # c. Eliminar signos (dejamos solo letras)\n",
        "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
        "\n",
        "    # d. Tokenización y Lematización (reducción a la raíz)\n",
        "    palabras = texto.split()\n",
        "    palabras_limpias = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in palabras\n",
        "        if word not in stop_words and len(word) > 2\n",
        "    ]\n",
        "\n",
        "    return \" \".join(palabras_limpias)\n",
        "\n",
        "print(\"Cargando dataset...\")\n",
        "\n",
        "df = pd.read_csv(\"dataset_react.csv\")\n",
        "\n",
        "print(\"Limpiando textos...\")\n",
        "df['clean_text'] = df['full_text'].apply(limpiar_texto)\n",
        "\n",
        "# Verificación\n",
        "\n",
        "print(\"\\n--- RESULTADO ---\")\n",
        "print(\"Original:\", df['full_text'].iloc[0][:80])\n",
        "print(\"Limpio:  \", df['clean_text'].iloc[0][:80])\n",
        "\n",
        "# Guardar\n",
        "df.to_csv(\"dataset_react_procesado.csv\", index=False)\n",
        "print(\"\\n¡Archivo 'dataset_react_procesado.csv' guardado correctamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modelado y Representación Vectorial\n",
        "Convertimos el texto limpio en números usando **TF-IDF** (Term Frequency - Inverse Document Frequency).\n",
        "\n",
        "\n",
        "> ***NOTA:***   \n",
        " No guardamos la matriz en CSV porque es demasiado grande y compleja.\n",
        "La variable 'tfidf_matrix' y 'tfidf_vectorizer' se quedan en la memoria  de Colab listas para el siguiente paso (Clustering y Sentimiento)."
      ],
      "metadata": {
        "id": "kjZ_nmWgKuS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwQIj24I4mtL",
        "outputId": "1858363a-c5c0-400c-c097-b2fd8bacfa2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando matriz TF-IDF...\n",
            "\n",
            "--- RESULTADOS ---\n",
            "Tamaño de la matriz: (899, 5000)\n",
            "\n",
            " Primeras 10 palabras del vocabulario: ['ability' 'able' 'able prioritize' 'abort' 'aborted' 'absolutely'\n",
            " 'accept' 'accept meta' 'acceptable' 'accepted']\n",
            "\n",
            " Algunas palabras del medio: ['changelog' 'changelogaemp' 'changelogaemp blockquote' 'changelogstrong'\n",
            " 'changelogstrong href' 'changesh' 'changessummary' 'changessummary pthis'\n",
            " 'changing' 'channel']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. CARGAR DATOS\n",
        "\n",
        "# Cargamos el archivo limpio que acabamos de guardar\n",
        "df = pd.read_csv(\"dataset_react_procesado.csv\")\n",
        "\n",
        "# Eliminar filas vacías (por si la limpieza dejó algún texto en blanco)\n",
        "df = df.dropna(subset=['clean_text'])\n",
        "\n",
        "print(\"Generando matriz TF-IDF...\")\n",
        "\n",
        "# 2. CONFIGURACIÓN DEL VECTORIZADOR (RF-03)\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,      # Top 5000 palabras más importantes\n",
        "    ngram_range=(1, 2),     # Unigramas y Bigramas (ej: \"data\", \"data science\")\n",
        "    min_df=2                # Ignorar palabras que aparecen en menos de 2 documentos (ruido)\n",
        ")\n",
        "\n",
        "# 3. TRANSFORMACIÓN\n",
        "# Esto convierte el texto en una matriz\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "# 4. ANÁLISIS DEL RESULTADO\n",
        "print(\"\\n--- RESULTADOS ---\")\n",
        "print(f\"Tamaño de la matriz: {tfidf_matrix.shape}\")\n",
        "# El resultado será (Filas, Columnas) --- (Número de Issues, Número de Palabras)\n",
        "\n",
        "# Ejemplo de palabras que el modelo aprendio\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\n Primeras 10 palabras del vocabulario: {feature_names[:10]}\")\n",
        "print(f\"\\n Algunas palabras del medio: {feature_names[500:510]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Análisis de Sentimiento Supervisado\n",
        "Entrenamos una **Regresión Logística** para clasificar opiniones.\n",
        "> ***Nota Técnica:***   Dado el fuerte desbalance de clases (muchos positivos, pocos negativos), se configuró el modelo con `class_weight='balanced'`. Esto obliga al algoritmo a prestar más atención a las críticas y reportes de error (Clase Negativa), mejorando significativamente el *Recall*."
      ],
      "metadata": {
        "id": "K_yvF0rjLBef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REVUbqnr5FuI",
        "outputId": "95c19ff7-3813-4677-93cf-6eb056d0018b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando diccionario de sentimientos VADER...\n",
            "Etiquetando datos automáticamente...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribución de sentimientos detectados:\n",
            "sentimiento\n",
            "Positivo    625\n",
            "Negativo    236\n",
            "Neutral      38\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Entrenando Modelo de Regresión Logística...\n",
            "Evaluando modelo...\n",
            "\n",
            "--- REPORTE DE CLASIFICACIÓN (RF-04) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negativo       0.58      0.75      0.66        56\n",
            "     Neutral       0.10      0.20      0.13         5\n",
            "    Positivo       0.88      0.72      0.79       119\n",
            "\n",
            "    accuracy                           0.72       180\n",
            "   macro avg       0.52      0.56      0.53       180\n",
            "weighted avg       0.76      0.72      0.73       180\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- 1. GENERACIÓN DE ETIQUETAS (EL \"PROFESOR\" VADER) ---\n",
        "print(\"Descargando diccionario de sentimientos VADER...\")\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def obtener_sentimiento(texto):\n",
        "    score = sia.polarity_scores(texto)['compound']\n",
        "    # Reglas estándar de VADER:\n",
        "    if score >= 0.05:\n",
        "        return \"Positivo\"\n",
        "    elif score <= -0.05:\n",
        "        return \"Negativo\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "print(\"Etiquetando datos automáticamente...\")\n",
        "# Usamos el texto limpio para calificar\n",
        "df['sentimiento'] = df['clean_text'].apply(obtener_sentimiento)\n",
        "\n",
        "# Veamos cuántos de cada tipo encontró\n",
        "print(\"\\nDistribución de sentimientos detectados:\")\n",
        "print(df['sentimiento'].value_counts())\n",
        "\n",
        "# --- 2. PREPARACIÓN PARA EL MODELO SUPERVISADO (RF-04) ---\n",
        "# X = Nuestros datos matemáticos (Matriz TF-IDF del paso anterior)\n",
        "# y = La respuesta correcta (las etiquetas que acabamos de crear)\n",
        "X = tfidf_matrix\n",
        "y = df['sentimiento']\n",
        "\n",
        "# Dividimos: 80% para entrenar (estudiar), 20% para test (examen)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 3. ENTRENAMIENTO DEL MODELO (LOGISTIC REGRESSION) ---\n",
        "print(\"\\nEntrenando Modelo de Regresión Logística...\")\n",
        "modelo = LogisticRegression(max_iter=1000, class_weight=\"balanced\") # max_iter alto para asegurar convergencia\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "# --- 4. EVALUACIÓN Y MÉTRICAS (RF-04) ---\n",
        "print(\"Evaluando modelo...\")\n",
        "y_pred = modelo.predict(X_test)\n",
        "\n",
        "print(\"\\n--- REPORTE DE CLASIFICACIÓN (RF-04) ---\")\n",
        "# Esto muestra Accuracy y F1-Score\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Guardamos el dataset final con las etiquetas\n",
        "df.to_csv(\"dataset_react_con_sentimientos.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2  Modelado de Temas (Aprendizaje No Supervisado)\n",
        "Para descubrir las tendencias de discusión sin intervención humana, aplicamos el algoritmo **K-Means**.\n",
        "* **Objetivo:** Agrupar los documentos en **3 clústeres** semánticos basados en la similitud de sus vectores TF-IDF.\n",
        "* **Interpretación:** Esto permitirá distinguir entre discusiones de desarrollo, mantenimiento de infraestructura y reportes automatizados."
      ],
      "metadata": {
        "id": "J3Q-m04wGX0i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNIlOxfM5df3",
        "outputId": "5b54407f-0860-4fa6-dd0b-f288c1713380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agrupando los textos en 3 temas...\n",
            "\n",
            "--- TEMAS IDENTIFICADOS ---\n",
            "\n",
            "GRUPO 0:\n",
            "Palabras clave: yarn, test, yarn test, cla, please, sign, run yarn, run, change, contributor\n",
            "Ejemplo de texto: fix grammar error overviewmd thanks submitting appreciate spending time work change please provide e...\n",
            "\n",
            "GRUPO 1:\n",
            "Palabras clave: compiler, using, bug, test, version, error, function, example, const, component\n",
            "Ejemplo de texto: bump webpack bump webpack detail summaryrelease notessummary pemsourced href releasesaemp blockquote...\n",
            "\n",
            "GRUPO 2:\n",
            "Palabras clave: gzip, change, current gzip, base, facebook, current, greater, size change, change greater, change includes\n",
            "Ejemplo de texto: flag cleanup enablehalt shipped failure warning markdown notice dangerid dangeridstable comparing bb...\n",
            "\n",
            "¡Análisis de temas completado y guardado!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# --- 1. CONFIGURACIÓN DE K-MEANS (RF-05) ---\n",
        "# se pide que encuentre 3 temas principales\n",
        "num_clusters = 3\n",
        "\n",
        "print(f\"Agrupando los textos en {num_clusters} temas...\")\n",
        "km = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "\n",
        "# Entrenamos con la matriz TF-IDF\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "# Guardamos el grupo (cluster) al que pertenece cada texto\n",
        "df['cluster'] = km.labels_\n",
        "\n",
        "# --- 2. INTERPRETACIÓN DE LOS TEMAS ---\n",
        "print(\"\\n--- TEMAS IDENTIFICADOS ---\")\n",
        "# Obtenemos los centros de los clusters y las palabras\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "tema_nombres = {}\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(f\"\\nGRUPO {i}:\")\n",
        "    # Imprimimos las 10 palabras más importantes de ese grupo\n",
        "    top_words = [terms[ind] for ind in order_centroids[i, :10]]\n",
        "    print(f\"Palabras clave: {', '.join(top_words)}\")\n",
        "\n",
        "    # Imprimimos un ejemplo real de ese grupo para entenderlo mejor\n",
        "    ejemplo = df[df['cluster'] == i]['clean_text'].iloc[0]\n",
        "    print(f\"Ejemplo de texto: {ejemplo[:100]}...\")\n",
        "\n",
        "# --- 3. GUARDADO FINAL ---\n",
        "df.to_csv(\"dataset_react_completo.csv\", index=False)\n",
        "print(\"\\n¡Análisis de temas completado y guardado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgPej_M65tsM",
        "outputId": "f3ae3b2a-4dac-4279-b183-1faa0829349a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculando similitudes...\n",
            "\n",
            "--- MENSAJE ORIGINAL (ID: 10) ---\n",
            "Texto: devtools throw error attempting clone nonexistent node existing serialisation logic trace profiler p...\n",
            "Cluster: 1\n",
            "\n",
            "--- 3 MENSAJES MÁS SIMILARES ENCONTRADOS ---\n",
            "ID: 477 | Similitud: 0.2353 (23.5%)\n",
            "Texto: chore add nvmrc specify node development consistency summary add nvmrc file specifying node recommen...\n",
            "----------------------------------------\n",
            "ID: 321 | Similitud: 0.1744 (17.4%)\n",
            "Texto: upgrade github action node compatibility summary upgrade github action latest version ensure compati...\n",
            "----------------------------------------\n",
            "ID: 201 | Similitud: 0.1544 (15.4%)\n",
            "Texto: devtools clear element inspection host element owned renderer selected stacked click outside root el...\n",
            "----------------------------------------\n",
            "\n",
            "--- MENSAJE ORIGINAL (ID: 50) ---\n",
            "Texto: bug logcomponentrender break existing functionality utilise proxy trpc client clientproceduretype fu...\n",
            "Cluster: 1\n",
            "\n",
            "--- 3 MENSAJES MÁS SIMILARES ENCONTRADOS ---\n",
            "ID: 352 | Similitud: 0.2418 (24.2%)\n",
            "Texto: bug server component error directly rendering context server component render context directly resul...\n",
            "----------------------------------------\n",
            "ID: 36 | Similitud: 0.2336 (23.4%)\n",
            "Texto: flight allow context provider client module allows server component import context use client module...\n",
            "----------------------------------------\n",
            "ID: 31 | Similitud: 0.2195 (22.0%)\n",
            "Texto: fix logcomponentrender breaking proxy object like trpc client summary reacts performance tracking tr...\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. CÁLCULO DE LA MATRIZ DE SIMILITUD (RF-06) ---\n",
        "# Comparamos TODOS contra TODOS.\n",
        "\n",
        "print(\"Calculando similitudes...\")\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# --- 2. FUNCIÓN DE BÚSQUEDA ---\n",
        "# Esta función cumple el requisito: \"Dado un mensaje, mostrar los similares\"\n",
        "def recomendar_similares(id_texto, top_n=3):\n",
        "    # Obtenemos los puntajes de similitud para ese texto específico\n",
        "    sim_scores = list(enumerate(cosine_sim[id_texto]))\n",
        "\n",
        "    # Los ordenamos de mayor a menor similitud\n",
        "\n",
        "    # (El primero siempre es él mismo, así que lo saltamos con [1:top_n+1])\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:top_n+1]\n",
        "\n",
        "    # Imprimimos los resultados\n",
        "    print(f\"\\n--- MENSAJE ORIGINAL (ID: {id_texto}) ---\")\n",
        "    print(f\"Texto: {df['clean_text'].iloc[id_texto][:100]}...\")\n",
        "    print(f\"Cluster: {df['cluster'].iloc[id_texto]}\")\n",
        "\n",
        "    print(f\"\\n--- {top_n} MENSAJES MÁS SIMILARES ENCONTRADOS ---\")\n",
        "    for i, score in sim_scores:\n",
        "        print(f\"ID: {i} | Similitud: {score:.4f} ({(score*100):.1f}%)\")\n",
        "        print(f\"Texto: {df['clean_text'].iloc[i][:100]}...\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# --- 3. PROBAR CON UN EJEMPLO ---\n",
        "# recomendaciones para el texto número 10\n",
        "recomendar_similares(10)\n",
        "\n",
        "# SE rueba también con otro\n",
        "recomendar_similares(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Conclusiones y Hallazgos Técnicos\n",
        "Tras la ejecución del pipeline de análisis sobre el repositorio `facebook/react`, se presentan los siguientes hallazgos:\n",
        "\n",
        "1.  **Métrica de Desempeño (Recall):** El modelo supervisado alcanzó un **Recall del 75%** en la clase Negativa. Esto valida la eficacia del sistema como herramienta de alerta temprana para la detección de bugs.\n",
        "2.  **Identificación de Tendencias:** El algoritmo de Clustering reveló que la discusión técnica actual gira en torno al nuevo **React Compiler** (Grupo 1) y tareas de **Infraestructura/CI** (Grupo 0), separando eficazmente el ruido de los bots.\n",
        "3.  **Validación de Similitud:** El módulo de *Cosine Similarity* demostró capacidad para la deduplicación de incidencias, identificando reportes de error redundantes con alta precisión semántica."
      ],
      "metadata": {
        "id": "eHEdtp90GpPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  6. Recomendaciones\n",
        "Tras el análisis de los datos recolectados:\n",
        "1.  **Desbalance:** La comunidad tiende a ser muy constructiva (sentimiento mayoritariamente positivo/técnico).\n",
        "2.  **Detección de Problemas:** Gracias al ajuste de pesos (`balanced`), el modelo logra identificar reportes de fallos (issues negativos) que antes pasaban desapercibidos.\n",
        "3.  **Temas:** Los clusters separaron exitosamente el mantenimiento automático (bots) de las discusiones humanas sobre código."
      ],
      "metadata": {
        "id": "9DNv8B5bLHXg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}